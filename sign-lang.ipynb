{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Northwest\n",
    "\n",
    "\n",
    "\n",
    "## Tensorflow Sign Language Code Along: DNN, Dropout, Hyperparameters\n",
    "\n",
    "\n",
    "\n",
    "This Code Along will introduces using Tensorflow for image detection. In this code along, we will:\n",
    "\n",
    "- Load images and convert them to numpy arrays using PIL\n",
    "- Design Deep Neural Network (DNN) with a dropout layer.\n",
    "- Run DNN with different hyperparameter settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Import Tensorflow\n",
    "\n",
    "In the first input cell, we will import tensorflow and verify version used.\n",
    "\n",
    "Python execution of the cell should complete without any errors. You may get deprecated warnings, just ignore them. Let me try it below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print( tf.__version__ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need a few more libraries, so let's load them now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from PIL import Image\n",
    "import zipfile\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('gestures'):\n",
    "    ! wget https://github.com/tensorflow-northwest/sign-lang/raw/master/gestures.zip\n",
    "    with zipfile.ZipFile(\"gestures.zip\",\"r\") as zip_ref:\n",
    "        zip_ref.extractall(\".\")\n",
    "    ! rm gestures.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Source\n",
    "\n",
    "For each gesture, our dataset includes 1200 x 50x50pixel images in grayscale, stored in the gestures/ folder. The gestures/0/ folder contains 1200 blank images which signify \"none\" gesture.\n",
    "\n",
    "Let's go to the downloaded images and see what's there. Let's first check that we are in the correct directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pwd\n",
    "! ls -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see under the gestures directory, we have folders 0 through 26. Folder 0 is the none image, while folders 1 through 26 have images for the letters A..Z, respectively.\n",
    "\n",
    "Let's look under one of the folders (i.e., folder 1 for the letter A):\n",
    "\n",
    "<img src='gestures.jpg'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now read in a JPEG image and convert it to an uncompressed numpy 2D matrix of pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.asarray(Image.open(\"gestures/5/9.jpg\"))\n",
    "\n",
    "# NOTE: to save-\n",
    "# im = Image.fromarray(numpy.uint8(image))\n",
    "# im.save(\"image.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that the image is a numpy matrix as expected. Its type should be numpy.ndarray and it's shape should be 50 x 50 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( type(image) )\n",
    "print( image.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the image looks like in an array. As you can see, it is a matrix of values between 0 and 255 (uint8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting an Image\n",
    "\n",
    "Let's plot one of the images in the training set. To do so, we will use the plotting functions of matplotlib. Let's start by importing the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This line is specific to python notebooks (not python). \n",
    "# It causes plots to automatically be rendered (displayed) without issuing a show command.\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot it now\n",
    "plt.imshow( image )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset\n",
    "\n",
    "The function below traverses the gestures folder for each subfolder (letter in the alphabet) and loads the images and corresponding label (letter), which is obtained from the subfolder name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset( verbose = False ):\n",
    "    \"\"\" Load the Sign Language dataset for the Alphabet \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    # Process each subfolder (0..26) in the gestures folder\n",
    "    for subfolder in os.listdir(\"gestures\"):\n",
    "        if verbose == True: print(\"Loading Subfolder\", subfolder)\n",
    "        # There are 1200 images per letter\n",
    "        for i in range(1200):\n",
    "            # Read each image in\n",
    "            # image = cv2.imread(\"gestures/\"+ subfolder + \"/\" + str(i+1) + \".jpg\", 0)\n",
    "            image = np.asarray(Image.open(\"gestures/\"+ subfolder + \"/\" + str(i+1) + \".jpg\"))\n",
    "            # if bad image, skip\n",
    "            if np.any(image == None):\n",
    "                continue\n",
    "            # add image to images list\n",
    "            images.append( image )\n",
    "            # add corresponding label to labels list\n",
    "            labels.append( subfolder )\n",
    "    # return the list of images and corresponding labels\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now load the images. We will set the parameter verbose to True to see the progress since this will take awhile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "images, labels = load_dataset( True )\n",
    "print(\"Time to Load Images: \"+ str(1000*(time.time() - start))+\" ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see we got what we expected. There are 27 folders (26 letters and None) with 1200 images each. We should have a list of 32,400 (27 * 1200) numpy 2D matrixes (aka the images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( len(images) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now verify that the images (and labels) we read in by plotting one of the images and it's corresponding label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot it now\n",
    "index = 2407\n",
    "plt.imshow( images[index] )\n",
    "print (\"y = \" + str(labels[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten the Image Data\n",
    "\n",
    "Now we will flatten the data (reshape so all rows follow each other sequential in a single vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert python list to numpy array\n",
    "images_np = np.asarray(images)\n",
    "print( \"Images as Numpy Array:\", images_np.shape )\n",
    "\n",
    "images_flatten = images_np.reshape(images_np.shape[0], -1)\n",
    "print( \"Images Flatten:\", images_flatten.shape )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the Image Data\n",
    "\n",
    "Now we will normalize the pixel values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the Pixel Values between 0 and 1\n",
    "x = images_flatten / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Conversion of Labels\n",
    "\n",
    "Let's now convert the labels from a categorical value to a one hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert python list to numpy array\n",
    "labels_np = np.asarray(labels)\n",
    "print( \"Labels as Numpy Array:\", labels_np.shape )\n",
    "\n",
    "# Let's look at the array data type\n",
    "labels_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's an unsigned 64bit integer. For our purposes we need it as a signed integer. Let's change its datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_int = labels_np.astype(np.int64)\n",
    "\n",
    "# Let's look at the new data type\n",
    "labels_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the array shape and data type are ready. Let's do the one hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels_to_one_hot_encoding(Y, C):\n",
    "    \"\"\" This function will do the reshape and conversion (from Coursera)\"\"\"\n",
    "    Y = np.eye(C)[Y.reshape(-1)]\n",
    "    return Y\n",
    "\n",
    "# Let's do the conversion\n",
    "y = convert_labels_to_one_hot_encoding(labels_int, 27)\n",
    "\n",
    "# Let's print the shape of our labels\n",
    "print( y.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Dataset into Training and Test Data\n",
    "\n",
    "Now that we have our data and labels ready, let's split our dataset into training and test data. For our purposes, we will use 80% as training and 20% as test. We could do the split by hand using numpy.random.shuffle() to randomize the order, and then use array slicing.\n",
    "\n",
    "But scikit-learn has an all-in-one method for this purpose, so we will use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split arrays or matrices into random train and test subsets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split our data/labels into 80% training and 20% test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network with Dropout Layer - Softmax Activation\n",
    "\n",
    "We are going to build a simple deep neural network (DNN) with a dropout layer. There will not be a convolutional layer. That is, we will use all 2500 pixels as input. The output layer from our neural network will be passed through a softmax activation function to produce our predictions of the letter.\n",
    "\n",
    "In our Neural Network, we will have the following:\n",
    "\n",
    "    - An input layer of 2500 inputs and 64 outputs\n",
    "    - A linear recitifier activation function\n",
    "    - A first hidden layer of 64 inputs and 32 outputs\n",
    "    - A dropout layer\n",
    "    - A linear recitifier activation function\n",
    "    - A second hidden layer of 32 inputs and 20 outputs\n",
    "    - A linear recitifier activation function\n",
    "    - An output layer of 20 inputs and 27 outputs\n",
    "    - A softmax activation function\n",
    "\n",
    "INPUT LAYER => RELU => HIDDEN LAYER => DROPOUT LAYER => RELU => HIDDEN LAYER => RELU => OUTPUT LAYER => SOFTMAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "\n",
    "We will use the softmax layer to make our predictions. Each output from softmax will be a number between 0 and 1, representing a percent. That is, if the output for the node 3 is 0.8, then this means 80% prediction. We will choose the output with the highest percent when making a prediction.\n",
    "\n",
    "Softmax is a mathematical function that takes a set of values, which may otherwise not add up to 1, and outputs a new set of numbers when totaled will add up to 1. That is, we use softmax() so that all our outputs for each image add up to 1 (100%).\n",
    "\n",
    "Softmax will be our 'activation' function from the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building your first neural network in TensorFlow\n",
    "\n",
    "### Design, then Run\n",
    "\n",
    "#### Design\n",
    "\n",
    "     -- Create placeholders for your input data\n",
    "     -- Design the layers as a Graph\n",
    "     -- Set the optimizer\n",
    "     \n",
    "#### Run\n",
    "\n",
    "      -- Initialize the Graph\n",
    "      -- Set number of epochs\n",
    "      -- Set batch size, learning rate\n",
    "      -- Run the Graph with the Training Data to Train a Model\n",
    "      -- Validate the Model with Test Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Vector and Output Vector and Hyperparameter Placeholders\n",
    "\n",
    "For our first tensorflow step, we will setup Tensorflow placeholders.\n",
    "\n",
    "We have two placeholders we need to declare, one for the input vector (pixel image data) and one for the output vector (letter classifier).\n",
    "\n",
    "For our input placeholder (which we call X), we have 2,500 features (pixels per image). For the output vector (which we call Y), we have have 27 classifiers (alphabet and None). In both cases, we set the second dimension of our vector to None. The None is a placeholder for the number of samples we will feed into the neural network. We also know that our data is floating point values between 0 and 1, so we will set the data type to float32.\n",
    "\n",
    "We will declare two more placeholders for setting some hyper-parameters, the percent to keep in the dropout layer (D, and the learning rate in the optimizer (L). Since both are scalar values, we will define their shape as a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first reset our graph, so our neural network components are all declared within the same graph\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[2500, None])\n",
    "Y = tf.placeholder(tf.float32, shape=[27, None])\n",
    "D = tf.placeholder(tf.float32, [])\n",
    "L = tf.placeholder(tf.float32, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INPUT LAYER\n",
    "\n",
    "Let's now design our input layer. We need two things: weights and biases. \n",
    "\n",
    "Each input feature (pixel) will need a weight (which our model will learning during training). The weight is multipled against the value of the input (pixel), which we symbolically represent as Wx. \n",
    "\n",
    "Each output from the layer will need a bias (which our model will learning during training). The bias is added to the result of the weight multipled by the pixel value (Wx).\n",
    "\n",
    "Let's create two Tensorflow variables for our weights and biases. The weights (which we call W) will need to be a 2D matrix. The rows are the number of inputs, which is 2500 and the columns the number of outputs to the hidden layer, which will be 64.\n",
    "\n",
    "The bias will be a vector of size 64 (one for each output).\n",
    "\n",
    "We need to initialize our weights and biases to some initial value. We will initialize the weights using a random value initializer (Xavier) and initialize the biases to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(1)   # Set the same seed to get the same initialization as in this demo.\n",
    "\n",
    "# The weights for the input layer\n",
    "W1 = tf.get_variable(\"W1\", [64, 2500], initializer=tf.contrib.layers.xavier_initializer(seed=1))\n",
    "\n",
    "# The bias for the output from the input layer\n",
    "b1 = tf.get_variable(\"b1\", [64, 1], initializer=tf.zeros_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put it together into an input layer. We will use the Tensorflow method tf.matmul() to do a matrix multiplication of the weights (our variable W1) and the inputs (our placeholder X), add in the bias (b1), and pass the output through a linear activation function.\n",
    "\n",
    "- Create a node that will multiply the weights (W1) against the input vector (X - which is our input placeholder).\n",
    "- Create a node that adds the bias to the above node (W1 * X)\n",
    "- Pass the outputs from the input layer through a RELU activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first layer (input layer)\n",
    "Z1 = tf.add(tf.matmul(W1, X), b1)\n",
    "\n",
    "# Let's add the activation function to the output signal from the first layer\n",
    "A1 = tf.nn.relu(Z1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIRST HIDDEN LAYER\n",
    "\n",
    "The first hidden layer will have 64 inputs (outputs from input layer) and 32 outputs. Each input will need a weight and each output a bias (which we will train). Each output will be passed through the linear rectifier unit (RELU) activation function.\n",
    "\n",
    "We will initialize the weights using a random value initializer (Xavier) and initialize the biases to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = tf.get_variable(\"W2\", [32, 64], initializer=tf.contrib.layers.xavier_initializer(seed=1))\n",
    "b2 = tf.get_variable(\"b2\", [32, 1], initializer=tf.zeros_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's construct the first hidden layer\n",
    "\n",
    "- Create a node that will multiply the weights (W2) against the outputs of the input layer (A1).\n",
    "- Create a node that adds the bias to the above node (W2 * A1)\n",
    "- Pass the outputs from the (first) hidden layer through a dropout layer\n",
    "- Pass the outputs from the dropout layer through a RELU activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The second layer (first hidden layer)\n",
    "Z2 = tf.add(tf.matmul(W2, A1), b2) \n",
    "\n",
    "# Let's add the dropout layer to the output signal from the second layer\n",
    "D2 = tf.nn.dropout(Z2, keep_prob=D)\n",
    "\n",
    "# Let's add the activation function to the output signal from the dropout layer\n",
    "A2 = tf.nn.relu(D2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SECOND HIDDEN LAYER\n",
    "\n",
    "The second hidden layer will have 32 inputs (outputs from first hidden layer) and 20 outputs. Each input will need a weight and each output a bias (which we will train). Each output will be passed through the linear rectifier unit (RELU) activation function.\n",
    "\n",
    "We will initialize the weights using a random value initializer (Xavier) and initialize the biases to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W3 = tf.get_variable(\"W3\", [20, 32], initializer=tf.contrib.layers.xavier_initializer(seed=1))\n",
    "b3 = tf.get_variable(\"b3\", [20, 1], initializer=tf.zeros_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's construct the second hidden layer.\n",
    "- Create a node that will multiply the weights (W3) against the outputs of the first hidden layer (A2).\n",
    "- Create a node that adds the bias to the above node (W3 * A2)\n",
    "- Pass the outputs from the second hidden layer through a RELU activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The third layer (second hidden layer)\n",
    "Z3 = tf.add(tf.matmul(W3, A2), b3) \n",
    "\n",
    "# Let's add the activation function to the output signal from the third layer\n",
    "A3 = tf.nn.relu(Z3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OUTPUT LAYER\n",
    "\n",
    "The output layer will have 20 inputs (outputs from the second hidden layer) and 27 outputs (one for each letter and None). Each input will need a weight and each output a bias (which we will train). The 27 outputs will be passed through a softmax activation function. \n",
    "\n",
    "We will initialize the weights using a random value initializer (Xavier) and initialize the biases to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W4 = tf.get_variable(\"W4\", [27, 20], initializer=tf.contrib.layers.xavier_initializer(seed=1))\n",
    "b4 = tf.get_variable(\"b4\", [27, 1], initializer=tf.zeros_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's construct the output layer.\n",
    "\n",
    "- Create a node that will multiply the weights (W4) against the outputs of the second hidden layer (A3).\n",
    "- Create a node that adds the bias to the above node (W4 * A3)\n",
    "- Pass the outputs from the output layer through a SOFTMAX squashing function (done by the optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fourth layer (output layer)\n",
    "Z4 = tf.add(tf.matmul(W4, A3), b4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIMIZER\n",
    "\n",
    "Now its time to design our optimizer. Let's start by designing our cost function. We will use the mean value of the softmax cross entropy between the predicted labels and actual labels. This is what we want to reduce on each batch (aka the cost)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=tf.transpose(Z4), labels=tf.transpose(Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's design our optimizer. This is the method that adjusts the values of the weights and biases, based on minizing the cost value during training.\n",
    "\n",
    "We also need to set a learning rate. This is multiplied against the gradient calculation. It's used to prevent huge swings in setting weights which can result in either converging at a local (instead of global) optima, or not converging at all (infinite gradient). We will set the learning rate when we run the graph using the placeholder L."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The learning rate for Gradient Descent algorithm\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(L).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Graph\n",
    "\n",
    "We've built our Tensorflow graph for training our data. So, let's start training it.\n",
    "\n",
    "First, we need to call Tensorflow's global_variables_initializer() method to initialize the variables we've defined. We will create this as another node, which will be the first node we run (evaluate) in our graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's set our hyperparameters.\n",
    "\n",
    "We need to set the number of epochs (that's how many times we run the training data through the neural network), and the batch size. The batch size is a small subset of the entire training set. We will be running a batch at a time per epoch. After each batch, then the cost is computed and backpropagated through the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20                                    # run a 20 epochs\n",
    "\n",
    "batch_size = 100                               # for each epoch, train in batches of 100 images\n",
    "\n",
    "number_of_images = x_train.shape[0]            # number of images in training data\n",
    "\n",
    "batch_size = 200                               # size of each batch\n",
    "\n",
    "# Feed Dictionary Parameters\n",
    "\n",
    "keep_prob = 0.9                                # percent of outputs to keep in dropout layer\n",
    "\n",
    "learning_rate = 0.5                            # the learning rate for graident descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to run the graph now!\n",
    "\n",
    "We start by creating a tensorflow session (tf.Session()). Within the session we can run (evaluate) parts of the graph we designed.\n",
    "\n",
    "We start by initializing the tensor variables we defined for the weights and biases.\n",
    "\n",
    "We then run our training data through our neural network for the number of epochs we defined. For each epoch, we get a randomly shuffled batch from the training data and feed the batch (i.e. feed dictionary) into the neural network by running (evaluate) the optimizer node in our graph.\n",
    "\n",
    "Once we've trained the model, then we create some new nodes to calculate accuracy and evaluate against the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    start = time.time()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize the variables\n",
    "        sess.run(init)\n",
    "        \n",
    "        # number of batches in an epoch\n",
    "        batches = number_of_images // batch_size\n",
    "\n",
    "        # run our training data through the neural network for each epoch\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "          epoch_cost = 0\n",
    "\n",
    "          # Run the training data through the neural network\n",
    "          for batch in range(batches):\n",
    "\n",
    "              # Calculate the start and end indices for the next batch\n",
    "              begin = (batch * batch_size)\n",
    "              end   = (batch * batch_size) + batch_size\n",
    "\n",
    "              # Get the next sequential batch from the training data\n",
    "              batch_xs, batch_ys = x_train[begin:end], y_train[begin:end]\n",
    "\n",
    "              # Feed this batch through the neural network.\n",
    "              _, batch_cost = sess.run([optimizer, cost], feed_dict={X: batch_xs.T, Y: batch_ys.T, D: keep_prob, L: learning_rate})\n",
    "\n",
    "              epoch_cost += batch_cost\n",
    "\n",
    "          print(\"Epoch: \", epoch, \"   Cost: \", epoch_cost / batches)\n",
    "\n",
    "        print(\"Training Time: \"+ str(1000*(time.time() - start))+\" ms\")\n",
    "        # Test the Model\n",
    "\n",
    "        # Let's select the highest percent from the softmax output per image as the prediction.\n",
    "        prediction = tf.equal(tf.argmax(Z4), tf.argmax(Y))\n",
    "\n",
    "        # Let's create another node for calculating the accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))\n",
    "\n",
    "        # Now let's run our trainingt images through the model to calculate our accuracy during training\n",
    "        # Note how we set the keep percent for the dropout rate to 1.0 (no dropout) when we are evaluating the accuracy.\n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: x_train.T, Y: y_train.T, D: 1.0}))\n",
    "\n",
    "        # Now let's run our test images through the model to calculate our accuracy on the test data\n",
    "        print (\"Test Accuracy:\", accuracy.eval({X: x_test.T, Y: y_test.T, D: 1.0}))\n",
    "        \n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model\n",
    "\n",
    "The last three steps above is where our test data was ran through the model and produced how accurate our model was on the test data.\n",
    "\n",
    "After training the model, we created a node for prediction. This node compares two vectors, our predicted labels and our actual labels. Each vector is 10 elements long with a 1 in the predicted/actual digit location. So we are comparing the vectors. If they match (prediction matches actual), then we have a TRUE; otherwise a FALSE. That's how we are going to get our accuracy percentage calculated.\n",
    "\n",
    "Next, we create the node accuracy. This node is a cost function!\n",
    "\n",
    "We then run the accuracy node, feeding it the test images as the X variable and the test labels as the Y variable. This will result in the test images being ran through the model (which is in memory) and the corresponding output vectors evaluated against the actual labels of the test images (Y)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rerun with different learning rate\n",
    "\n",
    "Wah! We only got 3% accuracy with our learning rate at 0.5\n",
    "\n",
    "It appears we were converging, but more likely wildly bouncing back and forth on reducing the cost of each batch. This might be an infinite gradient problem. Let's try lowering the learning are by a magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.05\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rerun with different dropout\n",
    "\n",
    "Wow, we got 99.9% accuracy! We probably won't do better. The training and test accuracy are nearly the same, so we do not have an overfitting problem. But for the fun of it, let's increase the dropout rate and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob = 0.5\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not really any difference!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try lowering our batch size by a magnitude. This will cause the optimizer (backward probagation) to run 10 times more often. This will cause it to take longer. Perhaps an increase in accuracy will make it worthwhile?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it took double the time. Our training accuracy whent to 100% - Wow. But the test accuracy is still the same. Looks like the only difference was it took twice as long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You have now successful designed and run DNN with Dropout, and feeding hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
